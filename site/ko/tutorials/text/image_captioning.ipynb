{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2s1A9eLRPEj"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "VRLVEKiTEn04"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cffg2i257iMS"
   },
   "source": [
    "# 시각적 초점을 통한 이미지 캡션\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/image_captioning\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    TensorFlow.org에서 보기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ko/tutorials/text/image_captioning.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ko/tutorials/text/image_captioning.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    GitHub 소스 보기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/ko/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />노트북 다운로드하기</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QASbY_HGo4Lq"
   },
   "source": [
    "우리의 목표는 아래의 예시 이미지가 주어졌을 때, \"파도를 타는 서퍼\"와 같은 캡션을 만드는 것 입니다.\n",
    "\n",
    "![Man Surfing](https://tensorflow.org/images/surf.jpg)\n",
    "\n",
    "*[이미지 소스](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg); 라이선스 : 공개 라이선스*\n",
    "\n",
    "작업을 완수하기 위해 모델이 캡션을 생성할 때 어떤 부분에 초점을 맞추는지 확인할 수 있는 초점 기반 모델을 사용할 것 입니다.\n",
    "\n",
    "![Prediction](https://tensorflow.org/images/imcap_prediction.png)\n",
    "\n",
    "이 모델 구조는 [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)와 유사합니다.\n",
    "\n",
    " 이 노트북은 End-to-End 방식 학습 예제입니다. 노트북을 실행하면, [MS-COCO](http://cocodataset.org/#home) 데이터 세트를 다운로드 하고, Inception V3를 사용하여 이미지의 일부를 전처리 및 캐시하고, 인코더-디코더 모델을 훈련시키고, 그 훈련된 모델을 사용하여 새로운 이미지에 대한 캡션을 생성합니다.\n",
    "\n",
    "아래의 예제에서는 약 20,000개의 이미지에 대한 30,000개의 캡션 정도의 비교적 적은 양의 데이터에 대한 모델을 훈련할 것 입니다(하나의 이미지당 여러 개의 캡션이 있기 때문에)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6svGEEek67ds"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f83b66c5be34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# 캡션하는 동안 모델이 이미지의 어느 부분에 초점을 맞추는지 확인하기 위해\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# 초점플롯을 생성합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version은 오직 코랩에만 존재합니다.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "# 캡션하는 동안 모델이 이미지의 어느 부분에 초점을 맞추는지 확인하기 위해\n",
    "# 초점플롯을 생성합니다.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn은 많은 유용한 유틸리티를 포함합니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qbGw8MRPE5"
   },
   "source": [
    "## MS-COCO 데이터세트 다운로드 및 준비\n",
    "\n",
    "모델을 훈련시키기 위해 [MS-COCO 데이터세트](http://cocodataset.org/#home)를 사용합니다. 이 데이터 세트는 각각 최소 5개의 캡션을 가진 82,000개 이상의 이미지를 가지고 있습니다. 아래 코드는 데이터세트를 자동으로 다운로드 및 추출합니다.\n",
    "\n",
    "**주의: 대용량 다운로드**. 훈련 세트 파일의 용량은 13GB입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krQuPYTtRPE7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-87f9a3c058e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m annotation_zip = tf.keras.utils.get_file('captions.zip',\n\u001b[0m\u001b[0;32m      2\u001b[0m                                           \u001b[0mcache_subdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                           \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                           extract = True)\n\u001b[0;32m      5\u001b[0m \u001b[0mannotation_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotation_zip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/annotations/captions_train2014.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.'),\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "\n",
    "name_of_zip = 'train2014.zip'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
    "  image_zip = tf.keras.utils.get_file(name_of_zip,\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "  PATH = os.path.abspath('.')+'/train2014/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aANEzb5WwSzg"
   },
   "source": [
    "## 훈련 세트의 크기 제한\n",
    "본 튜토리얼에 대한 훈련을 가속하기 위해, 30,000개의 캡션과 그에 상응하는 이미지를 사용해 모델을 훈련시킬 것 입니다. 더 많은 데이터를 선택할수록 캡션의 품질이 향상될 것 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G3b8x8_RPFD"
   },
   "outputs": [],
   "source": [
    "# json 파일을 읽습니다.\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# 벡터에 캡션과 이미지 이름을 저장\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# 이미지 이름과 캡션을 함께 셔플합니다\n",
    "# 임의의 상태로 설정합니다\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "\n",
    "# 셔플된 세트의 상위 30,000 캡션만 선택합니다\n",
    "num_examples = 30000\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPBMgK34RPFL"
   },
   "outputs": [],
   "source": [
    "len(train_captions), len(all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "## InceptionV3를 사용한 이미지 전처리\n",
    "\n",
    "다음으로, 이미넷에 사전 훈련된 InceptionV3를 사용해 각 이미지를 분류합니다. 마지막 컨볼루셔널 레이어에서 특성을 추출합니다.\n",
    "\n",
    "첫번째로, 이미지를 InceptionV3가 요구하는 형식으로 이미지를 변환 합니다 :\n",
    "* 이미지 사이즈를 299px * 299px로 재설정\n",
    "*  InceptionV3를 훈련하는데 사용되는 이미지의 형식과 일치하는 -1 ~ 1 범위의 픽셀을 포함하도록 정규화하는 [전처리 입력](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input) 기능을 통해 [이미지 전처리](https://cloud.google.com/tpu/docs/inception-v3-advanced#preprocessing_stage)를 진행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXR0217aRPFR"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDvIu4sXRPFV"
   },
   "source": [
    "## InceptionV3 초기화 및 사전 훈련된 ImageNet 가중치 로드\n",
    "이제 InceptionV3 아키텍처의 출력 레이어의 마지막 컨볼루셔널 레이어인 tf.keras 모델을 생성할 것 입니다. 이 레이어의 출력 형태는 ```8x8x2048``` 입니다. 이 예제에서는 '초점' 을 사용 하기 때문에 마지막 컨볼루셔널 레이어를 사용합니다. 이러한 초기화는 병목 현상을 유발할 수 있으므로 훈련중에는 초기화하지 않습니다.\n",
    "\n",
    "* 각 이미지를 네트워크를 통해 전달하고 결과 벡터를 딕셔너리에 저장합니다(image_name --> feature_vector).\n",
    "* 모든 이미지가 네트워크를 통해 전달되면, 딕셔너리를 선택하고 디스크에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD3vW4SsRPFW"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rERqlR3WRPGO"
   },
   "source": [
    "## InceptionV3 에서 추출한 특성 캐싱\n",
    "InceptionV3로 각 이미지를 전처리하고, 출력을 디스크에 캐싱합니다. RAM에서 출력을 캐싱하는 것은 더 빠를 뿐만 아니라, 메모리 집약적이어서 이미지당 8 \\* 8 \\* 2048 의 'float'을 요구합니다. 이것은 Colab의 메모리 한계를 초과합니다(현재 12GB).\n",
    "보다 정교한 캐싱 전략(랜덤 액세스 디스크 I/O를 줄이기 위해 이미지를 샤딩 하는 등)을 통해 성능을 개선할 수 있지만, 그렇게 하기 위해서는 더 많은 코드를 필요로 할 것 입니다.\n",
    "\n",
    "캐싱은 GPU와 함께 Colab에서 약 10분가량 실행 되는데, 이때 진행 표시줄을 보고싶다면 다음을 참고하세요 :\n",
    "\n",
    "1. [tqdm](https://github.com/tqdm/tqdm) 설치 :\n",
    "\n",
    "    `!pip install tqdm`\n",
    "\n",
    "2. tqdm 패키지 가져오기 :\n",
    "\n",
    "    `from tqdm import tqdm`\n",
    "\n",
    "3. 다음의 라인 변경 :\n",
    "\n",
    "    `for img, path in image_dataset:`\n",
    "\n",
    "    에서 아래와 같이:\n",
    "\n",
    "    `for img, path in tqdm(image_dataset):`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dx_fvbVgRPGQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_name_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e06e6cef6ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 고유한 이미지 얻기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencode_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 시스템 구성에 따라 batch_size를 자유롭게 조절할 수 있습니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_name_vector' is not defined"
     ]
    }
   ],
   "source": [
    "# 고유한 이미지 얻기\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# 시스템 구성에 따라 batch_size를 자유롭게 조절할 수 있습니다\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyqH3zFwRPFi"
   },
   "source": [
    "## 캡션 전처리 및 토큰화\n",
    "\n",
    "* 먼저 캡션을 토큰화 하세요(예 : 공간 분할). 이는 데이터의 모든 고유한 단어들의 어휘를 제공합니다. (예 : \"서핑\", \"풋볼\" 등)\n",
    "* 다음으로, 어휘의 사이즈를 상위 5,000개로 제한합니다. (메모리 절약을 위해).\n",
    "* 다른 모든 단어들을 \"알 수 없는(Unknown)\" 토큰으로 대체합니다..\n",
    "* 이후 단어-인덱스 및 인덱스-단어 매핑을 생성합니다.\n",
    "* 마지막으로, 모든 시퀀스를 가장 긴 시퀀스와 동일한 길이로 패딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZfK8RhQRPFj"
   },
   "outputs": [],
   "source": [
    "# 데이터 세트 내의 최대 길이 캡션 찾기\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJGE34aiRPFo"
   },
   "outputs": [],
   "source": [
    "# 단어들 중 상위 5,000개 단어 선택\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fpJb5ojRPFv"
   },
   "outputs": [],
   "source": [
    "# 토큰화된 벡터 생성\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AidglIZVRPF4"
   },
   "outputs": [],
   "source": [
    "# 각 벡터를 캡션의 max_length 로 패딩\n",
    "# max_length값을 제공하지 않을 경우, pad_sequences에서 자동으로 계산합니다.\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gL0wkttkRPGA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8ddae7abea8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 초점 가중치를 저장하는데 사용하는 max_length 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_max_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_seqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_seqs' is not defined"
     ]
    }
   ],
   "source": [
    "# 초점 가중치를 저장하는데 사용하는 max_length 계산\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3CD75nDpvTI"
   },
   "source": [
    "## 교육 및 테스트를 통한 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iS7DDMszRPGF"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c8f59e33bb5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 80-20 분할을 사용하여 훈련 및 검증 세트 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                                                     \u001b[0mcap_vector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                                     \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                     random_state=0)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# 80-20 분할을 사용하여 훈련 및 검증 세트 생성\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmViPkRFRPGH"
   },
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "## 훈련을 위한 tf.data 데이터 세트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "horagNvhhZiy"
   },
   "source": [
    "이미지와 캡션이 준비되었습니다! 다음으로, 모델 훈련에 사용할 tf.data 데이터 세트를 생성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3TnZ1ToRPGV"
   },
   "outputs": [],
   "source": [
    "# 시스템 구성에 따라 매개변수를 자유롭게 변경할 수 있습니다.\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# InceptionV3에서 추출한 벡터의 형태는 (64, 2048)\n",
    "# 두 변수는 벡터 형태를 나타냅니다.\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmZS2N0bXG3T"
   },
   "outputs": [],
   "source": [
    "# Numpy 파일 로드\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDF_Nm3tRPGZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-df43b8a75afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 맵을 사용하여 Numpy 파일을 병렬로 로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n\u001b[0;32m      5\u001b[0m           map_func, [item1, item2], [tf.float32, tf.int32]),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# 맵을 사용하여 Numpy 파일을 병렬로 로드\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 셔플 및 묶기\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrvoDphgRPGd"
   },
   "source": [
    "## 모델\n",
    "재미있는 사실 : 아래의 디코더는 [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb) 의 예제와 동일합니다.\n",
    "\n",
    "이 모델 아키텍처는 [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) 에 영감을 받아 제작되었습니다.\n",
    "\n",
    "* 이 예제에서, InceptionV3의 낮은 단계 컨볼루셔널 레이어에서 특성을 추출하여 (8, 8, 2048) 형태의 벡터를 제공합니다. \n",
    "* 그 벡터를 (64, 2048) 과 같은 형태로 밀어 넣습니다.\n",
    "* 이 벡터는 CNN 인코더(완전 연결 단일 레이어로 구성된)를 통해 전달됩니다.\n",
    "* RNN (여기서는 GRU) 는 다음 단어를 예측하기 위해 이미지를 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ja2LFTMSdeV3"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # 특성(CNN_encoder 출력) 형태 == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # 은닉 형태 == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis 형태 == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # 점수 형태 == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights 형태 == (batch_size, 64, 1)\n",
    "    # self.V에 점수를 적용하고 있기 때문에 마지막 축에서 1을 얻습니다.\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # 덧셈 후의 context_vector 형태 == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZ7R1RxHRPGf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9787f56fd0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCNN_Encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 이미 형상을 추출하고 pickle을 이용해 덤프하였으므로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 이 인코더는 완전 연결 레이어를 통해 특성을 전달합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCNN_Encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # 이미 형상을 추출하고 pickle을 이용해 덤프하였으므로\n",
    "    # 이 인코더는 완전 연결 레이어를 통해 특성을 전달합니다.\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # fc 후의 형태 == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9UbGQmERPGi"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # 초점을 별도의 모델로 정의\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # 임베딩 후의 x의 형태 == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 결합 후의 x의 형태 == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # 결합 벡터를 GRU로 전달\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # 형태 == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x의 형태 == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # 출력 형태 == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qs_Sr03wRPGk"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bYN7xA0RPGl"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6A3Ni64joyab"
   },
   "source": [
    "## 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpJAqPMWo0uE"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUkbqhc_uObw"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHod7t72RPGn"
   },
   "source": [
    "## 훈련\n",
    "\n",
    "* 각 `.npy` 파일에 저장된 특성을 추출한 후에 특성들을 인코더를 통해 전달합니다.\n",
    "* 인코더 출력, 은닉 상태(0으로 초기화된) 및 디코더 입력(시작 토큰)이 디코더로 전달 됩니다.\n",
    "* 디코더는 예측값과 은닉 상태 디코더를 반환합니다.\n",
    "* 은닉 상태 디코더는 다시 모델로 전달되고, 그 예측값을 통해 손실을 계산합니다.\n",
    "* 디코더의 다음 입력을 결정할 때는 Teacher-forcing 을 이용합니다.\n",
    "* Teacher-forcing 은 타깃 단어가 디코더에 다음 입력으로 전달되는 기술입니다.\n",
    "* 마지막 단계는 그래디언트를 계산하고 옵티마이저와 역전파에 적용하는 것 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vt4WZ5mhJE-E"
   },
   "outputs": [],
   "source": [
    "# 훈련 셀을 여러 번 실행하면 loss_plot 배열이 재설정되므로\n",
    "# 이를 별도의 분리된 셀에 추가합니다.\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqgyz2ANKlpU"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # 캡션은 이미지와는 관계가 없기 때문에\n",
    "  # 각 묶음의 은닉 상태를 초기화합니다.\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # 디코더를 통해 특성을 전달\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # Teacher-forcing 사용\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlA4VIQpRPGo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # 나중에 플롯할 에포크의 끝과 손실값을 저장합니다.\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wm83G-ZBPcC"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGvOcLQKghXN"
   },
   "source": [
    "## 캡션!\n",
    "\n",
    "* 평가 함수는 훈련 루프와 유사하지만, 여기서는 Teacher-forcing을 사용하지 않는다는 점을 제외합니다. 각 시간 단계에 따른 디코더에 대한 입력은 은닉 상태와 인코더 출력과 마찬가지로 이전의 예측입니다. \n",
    "* 모델이 최종 토큰을 예측하는 경우 예측을 중지하세요.\n",
    "* 그 다음 매 단계마다 초점 가중치를 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCWpDtyNRPGs"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD_y7PD6RPGt"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7x8RiPHe_4qI"
   },
   "outputs": [],
   "source": [
    "# 검증 세트의 캡션\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)\n",
    "# 이미지 열기\n",
    "Image.open(img_name_val[rid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rprk3HEvZuxb"
   },
   "source": [
    "## 자신의 이미지로 시도해보기\n",
    "재미로, 아래에서는 방금 훈련시킨 모델로 자신의 이미지를 캡션하는데 사용하는 방법을 제공했습니다. 상대적으로 적은 양의 데이터로 훈련되었으며, 당신의 이미지는 훈련 데이터와 다를 수 있다는 것을 명심하세요.(따라서 이상한 결과에 대비하세요!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "image_extension = image_url[-4:]\n",
    "image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "                                     origin=image_url)\n",
    "\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# 이미지 열기\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJZXyJco6uLO"
   },
   "source": [
    "# 다음 단계\n",
    "\n",
    "축하합니다! 드디어 초점 기반 이미지 캡션 모델을 훈련하셨습니다. 다음으로, 이 예제를 읽어보세요. [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb) 이것은 스페인어와 영어 문장을 번역하기 위해 비슷한 아키텍처를 사용합니다. 이 노트북의 코드를 다른 데이터 세트에서 훈련하는 실험을 해 볼 수도 있습니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K2s1A9eLRPEj"
   ],
   "name": "image_captioning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
